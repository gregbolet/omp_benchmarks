{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from benchmarks import *\n",
    "import glob\n",
    "import os, sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import product\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE = 'lassen' if 'lassen' in ROOT_DIR else 'ruby'\n",
    "print(MACHINE, ROOT_DIR)\n",
    "prognames = list(progs.keys())\n",
    "probsizes = ['smlprob', 'medprob', 'lrgprob']\n",
    "\n",
    "seeds = [1337, 3827, 9999, 4873]\n",
    "\n",
    "hypers = {\n",
    "\t'cma':(('popsize',), ('seed', 'sigma')),\n",
    "\t'pso':(('popsize', 'w'), ('seed', 'c1', 'c2')),\n",
    "\t'bo-ucb':(('seed',), ('kappa',)),\n",
    "\t'bo-ei': (('seed',), ('xi',)),\n",
    "\t'bo-poi': (('seed',), ('xi',))\n",
    "}\n",
    "\n",
    "#goMethods = list(hypers.keys())\n",
    "goMethods = ['pso', 'cma', 'bo']\n",
    "print(goMethods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbFile = f'{MACHINE}-fullExploreDataset.csv'\n",
    "xtimeDB = pd.read_csv(ROOT_DIR+'/databases/'+dbFile)\n",
    "\n",
    "globalOptimals = xtimeDB.groupby(['progname', 'probsize'])['xtime'].min().reset_index()\n",
    "\n",
    "print(globalOptimals)\n",
    "\n",
    "numthreads = 56 if MACHINE in 'ruby' else 80\n",
    "globalBaselines = xtimeDB.loc[(xtimeDB['OMP_NUM_THREADS'] == numthreads) \n",
    "\t\t\t\t\t\t\t\t\t\t& (xtimeDB['OMP_PROC_BIND'] == 'close')\n",
    "\t\t\t\t\t\t\t\t\t\t& (xtimeDB['OMP_PLACES'] == 'cores')\n",
    "\t\t\t\t\t\t\t\t\t\t& (xtimeDB['OMP_SCHEDULE'] == 'static'),['progname', 'probsize', 'xtime']]\n",
    "\n",
    "probsizeMap = {'smlprob': 'Small Problem', 'medprob': 'Medium Problem', 'lrgprob': 'Large Problem'}\n",
    "prognameMap = {'bt_nas': 'BT', 'ft_nas': 'FT', 'hpcg': 'HPCG', 'lulesh':'Lulesh'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallDF = pd.DataFrame()\n",
    "tojoin = []\n",
    "for progname in prognames:\n",
    "\tif progname == 'cg_nas' or progname == 'cfd_rodinia':\n",
    "\t\tcontinue\n",
    "\tfor method in goMethods:\n",
    "\t\t# read the pre-processed dataframes\n",
    "\t\tfilename = ROOT_DIR+'/databases/'+f'{MACHINE}-{progname}-{method}-GO_Data-rawXtimes.csv'\n",
    "\t\tfullDF = pd.read_csv(filename)\n",
    "\t\ttojoin += [fullDF]\n",
    "\n",
    "overallDF = pd.concat(tojoin, ignore_index=True, sort=True)\n",
    "overallDF = overallDF.drop(['optimXtime', 'kappa_decay', 'kappa_decay_delay'], axis=1)\n",
    "\n",
    "print(overallDF.columns)\n",
    "overallDF.loc[overallDF['method'] == 'bo', 'method'] = overallDF[overallDF['method'] == 'bo'].apply(lambda x: x['method']+'-'+x['utilFnct'], axis=1)\n",
    "overallDF = overallDF.drop(['utilFnct'], axis=1)\n",
    "\n",
    "for col in overallDF:\n",
    "\tif col == 'xtime' or col == 'globalSample' or col == 'optimXtime':\n",
    "\t\tcontinue\n",
    "\tprint(col, overallDF[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeGOHyperHeatmapPlots(df, method):\n",
    "\n",
    "\t# we're going to make one plot for each GO method\n",
    "\t# want to show programs in rows, probsize in columns\n",
    "\t# the heatmap for each will show the hyperparameters on the two axes\n",
    "\t\n",
    "\t# we're going to generate 4 heatmaps showing the following values:\n",
    "\t# 1) the earliest step a better-than-baseline configuration found\n",
    "\t# 2) the speedup of the best configuration found after 100 steps\n",
    "\t# 3) the speedup of the best configuration found after 200 steps\n",
    "\t# 4) the speedup of the best configuration found after 300 steps\n",
    "\t\n",
    "\t# get the subset we're interested in\n",
    "\tdf = df[df['method'] == method].reset_index(drop=True)\n",
    "\t\n",
    "\t#print(df.columns)\n",
    "\t\n",
    "\txaxis, yaxis = hypers[method]\n",
    "\t#print(xaxis, yaxis)\n",
    "\t\n",
    "\taxisCols = list(xaxis+yaxis)\n",
    "\taxisCols.remove('seed')\n",
    "\tcolsToKeep = ['progname', 'probsize', 'seed', 'globalSample']+axisCols\n",
    "\tcolsToDrop = list(df.columns)\n",
    "\t\n",
    "\t[colsToDrop.remove(col) for col in colsToKeep]\n",
    "\tcolsToDrop.remove('xtime')\n",
    "\t\n",
    "\t#print('dropping')\n",
    "\t#print(colsToDrop)\n",
    "\t\n",
    "\t# get rid of unused columns\n",
    "\tdf = df.drop(colsToDrop, axis=1)\n",
    "\t\n",
    "\t# presort the DF\n",
    "\tdf = df.sort_values(by=colsToKeep, ignore_index=True)\n",
    "\t\n",
    "\t# pick one combination to make column tuples with\n",
    "\tsubset = df[(df['progname'] == prognames[0]) & (df['probsize'] == probsizes[0]) & (df['seed'] == seeds[0]) & (df['globalSample'] == 0)]\n",
    "\t#print('subset cols', subset.columns)\n",
    "\t#print('subset shape',subset.shape)\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\t# need to make tuples of the columns, and stringify them\n",
    "\tif len(xaxis) > 1:\n",
    "\t\txtuples = list(product(*[list(sorted(subset[col].unique())) for col in xaxis]))\n",
    "\t\t#tuples = list(set([ str(a) for a in list(zip( *[list(subset[col]) for col in xaxis] ))]))\n",
    "\t\t#print(len(xtuples), xtuples)\n",
    "\t\n",
    "\t\t# tupleify the columns and drop them\n",
    "\t\tnewXColName = f'({\",\".join(xaxis)})'\n",
    "\t\tdf[newXColName] = list(zip(*[df[col] for col in xaxis]))\n",
    "\t\tdf = df.drop(list(xaxis), axis=1)\n",
    "\telse:\n",
    "\t\txtuples = list(sorted(subset[xaxis[0]].unique()))\n",
    "\t\tnewXColName = xaxis[0]\n",
    "\t\t#print(xtuples)\n",
    "\t\n",
    "\tif len(yaxis) > 1:\n",
    "\t\tytuples = list(product(*[list(sorted(subset[col].unique())) for col in yaxis]))\n",
    "\t\t#tuples = list(set([ str(a) for a in list(zip( *[list(subset[col]) for col in xaxis] ))]))\n",
    "\t\t#print(len(ytuples), ytuples)\n",
    "\t\n",
    "\t\t# tupleify the columns and drop them\n",
    "\t\tnewYColName = f'({\",\".join(yaxis)})'\n",
    "\t\tdf[newYColName] = list(zip(*[df[col] for col in yaxis]))\n",
    "\t\tdf = df.drop(list(yaxis), axis=1)\n",
    "\telse:\n",
    "\t\tytuples = list(sorted(subset[yaxis[0]].unique()))\n",
    "\t\tnewYColName = yaxis[0]\n",
    "\t\t#print(ytuples)\n",
    "\t\n",
    "\t#print('new columns')\n",
    "\t#print(df.columns)\n",
    "\t\n",
    "\t#print('uniques')\n",
    "\t## for each column, print the number of unique values\n",
    "\t#for col in list(df.columns):\n",
    "\t#\tif col != 'xtime':\n",
    "\t#\t\tprint(col, len(list(df[col].unique())))\n",
    "\t\n",
    "\t# find the min xtime found up to each globalSample\n",
    "\tdf['cummin'] = df.groupby([newXColName, newYColName, 'probsize', 'progname'])['xtime'].transform('cummin')\n",
    "\t\n",
    "\t# let's make a new dataframe column for each plot type we want to make\n",
    "\t# 1) earliest better-than-baseline config found for each GO hyperparam combination\n",
    "\t\n",
    "\t# rescale the xtime to be baseline-normalized\n",
    "\t#print('pre-reset index')\n",
    "\t#print(df.shape, df.head())\n",
    "\tdf = df.set_index(['progname', 'probsize', newXColName, newYColName])\n",
    "\tbaselines = globalBaselines.set_index(['progname', 'probsize'])\n",
    "\t#print('post- reset index')\n",
    "\t#print(df.shape, df.head())\n",
    "\t\n",
    "\tdf['baselineXtime'] = 1/df['cummin'].div(baselines.reindex(df.index)['xtime'], axis=0)\n",
    "\t\n",
    "\t#print('rescaled')\n",
    "\t#print(df.shape, df.head())\n",
    "\t\n",
    "\tearliestSamples = pd.DataFrame(index=df.index.copy())\n",
    "\t#earliestSamples = df.copy(deep=True)\n",
    "\t#print('super init earliest sampels')\n",
    "\t#print(earliestSamples.shape, earliestSamples)\n",
    "\tearliestSamples = earliestSamples[~earliestSamples.index.duplicated(keep='first')]\n",
    "\t#earliestSamples = earliestSamples.groupby(earliestSamples.index).first()\n",
    "\t\n",
    "\t# set it to the latest possible value\n",
    "\tearliestSamples['firstSample'] = 301\n",
    "\t\n",
    "\t#print('init early samples')\n",
    "\t#print(earliestSamples.shape, earliestSamples)\n",
    "\t\n",
    "\t#test = df.loc[df.baselineXtime >= 0.1, 'globalSample'].min()\n",
    "\ttest = df.loc[df.baselineXtime > 1.0].groupby(level=[0,1,2,3])['globalSample'].min()\n",
    "\t#print('found earliest')\n",
    "\t#print(test.shape, test)\n",
    "\t\n",
    "\t# now find the earliest globalSample that is >= 1.0\n",
    "\t# update only a couple elements\n",
    "\tearliestSamples['firstSample'].update(test)\n",
    "\t#print('earliest samples')\n",
    "\t#print(earliestSamples.shape, earliestSamples)\n",
    "\t\n",
    "\t# now reset the index\n",
    "\tearliestSamples = earliestSamples.reset_index()\n",
    "\t\n",
    "\t#print('reset index')\n",
    "\t#print(earliestSamples.shape, earliestSamples)\n",
    "\t\n",
    "\t\n",
    "\tdef drawHeatmap(*args, **kwargs):\n",
    "\t\tdata = kwargs.pop('data').copy(deep=True)\n",
    "\t\tif 'probsize' in list(data.columns):\n",
    "\t\t\tdata = data.drop(['probsize'], axis=1)\n",
    "\t\tif 'progname' in list(data.columns):\n",
    "\t\t\tdata = data.drop(['progname'], axis=1)\n",
    "\n",
    "\t\tdata = data.pivot(index=newYColName, columns=newXColName, values='firstSample')\n",
    "\n",
    "\t\tif method == 'pso':\n",
    "\t\t\tsns.heatmap(data, annot_kws={'fontsize':6}, **kwargs)\n",
    "\t\telse:\n",
    "\t\t\tsns.heatmap(data, **kwargs)\n",
    "\n",
    "\t\treturn\n",
    "\t\n",
    "\t# let's first make the tuples of columns\n",
    "\tg = sns.FacetGrid(earliestSamples, row='progname', col='probsize', col_order=probsizes, palette='flare', height=15, aspect=1.5)\n",
    "\tg.map_dataframe(drawHeatmap, annot=True, vmin=0, vmax=300, cbar=True, xticklabels=True, yticklabels=True, fmt='.3g')\n",
    "\t\n",
    "\tfor ax in g.axes.flatten():\n",
    "\t\tax.tick_params(axis='x', labelbottom=True, labelrotation=90)\n",
    "\t\n",
    "\tplt.tight_layout()\n",
    "\t\n",
    "\tg.fig.subplots_adjust(top=0.96)\n",
    "\tg.fig.suptitle(f'GO Hyperparam Exploration ({method.upper()}) -- Samples till >= baseline xtime')\n",
    "\t\n",
    "\tplt.show()\n",
    "\n",
    "\t# now let's make the plot for where the best is by step 100\n",
    "\t\n",
    "\t# let's make the plots showing what happens at each step 0, 50, 100, 150, 200, 250, 299\n",
    "\t# want the best speedup witnessed up until each point\n",
    "\n",
    "\tsamps = pd.DataFrame(index=df.index.copy())\n",
    "\tsamps = samps[~samps.index.duplicated(keep='first')]\n",
    "\n",
    "\t# update the dataframe with the desired columns\n",
    "\tfor i in range(0,7):\n",
    "\t\tsteps = 50*i\n",
    "\t\tsteps = 299 if steps == 300 else steps\n",
    "\t\tcolName = f'step{steps}'\n",
    "\t\tsamps[colName] = 0\n",
    "\t\ttest = df.loc[df.globalSample == steps].groupby(level=[0,1,2,3])['baselineXtime'].max()\n",
    "\t\tsamps[colName].update(test)\n",
    "\n",
    "\tsamps = samps.reset_index()\n",
    "\n",
    "\tdef drawHeatmap(*args, **kwargs):\n",
    "\t\tdata = kwargs.pop('data').copy(deep=True)\n",
    "\t\ttargetCol = kwargs.pop('step')\n",
    "\t\tif 'probsize' in list(data.columns):\n",
    "\t\t\tprobsize = data.iloc[0]['probsize']\n",
    "\t\t\tdata = data.drop(['probsize'], axis=1)\n",
    "\t\tif 'progname' in list(data.columns):\n",
    "\t\t\tprogname = data.iloc[0]['progname']\n",
    "\t\t\tdata = data.drop(['progname'], axis=1)\n",
    "\n",
    "\t\toptimalXtime = globalOptimals[(globalOptimals.progname == progname) & (globalOptimals.probsize == probsize)]['xtime'].iat[0]\n",
    "\t\tbaselineXtime = globalBaselines[(globalBaselines.progname == progname) & (globalBaselines.probsize == probsize)]['xtime'].iat[0]\n",
    "\n",
    "\t\tvmax = baselineXtime/optimalXtime\n",
    "\n",
    "\t\t#print(f'working on {probsize} {progname} vmax {vmax}')\n",
    "\n",
    "\t\tdata = data.pivot(index=newYColName, columns=newXColName, values=targetCol)\n",
    "\n",
    "\t\tif method == 'pso':\n",
    "\t\t\tsns.heatmap(data, vmax=vmax, norm=mcolors.TwoSlopeNorm(vcenter=1.0), cmap='seismic', annot_kws={'fontsize':6}, **kwargs)\n",
    "\t\telse:\n",
    "\t\t\tsns.heatmap(data, vmax=vmax, norm=mcolors.TwoSlopeNorm(vcenter=1.0), cmap='seismic', **kwargs)\n",
    "\n",
    "\t\treturn\n",
    "\n",
    "\n",
    "\t# let's first make the tuples of columns\n",
    "\tfor i in range(0,7):\n",
    "\t\tstep = 50*i\n",
    "\t\tstep = 299 if step == 300 else step\n",
    "\n",
    "\t\tg = sns.FacetGrid(samps, row='progname', col='probsize', col_order=probsizes, palette='flare', height=15, aspect=1.5)\n",
    "\t\tcolName = f'step{step}'\n",
    "\t\tg.map_dataframe(drawHeatmap, step=colName, annot=True, vmin=0.0, cbar=True, xticklabels=True, yticklabels=True)\n",
    "\n",
    "\t\tfor ax in g.axes.flatten():\n",
    "\t\t\tax.tick_params(axis='x', labelbottom=True, labelrotation=90)\n",
    "\n",
    "\t\tplt.tight_layout()\n",
    "\n",
    "\t\tg.fig.subplots_adjust(top=0.96)\n",
    "\t\tg.fig.suptitle(f'GO Hyperparam Exploration ({method.upper()}) -- Best Speedup at {step} Samples')\n",
    "\n",
    "\t\tplt.show()\n",
    "\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#makeGOHyperHeatmapPlots(overallDF, 'pso')\n",
    "#makeGOHyperHeatmapPlots(overallDF, 'cma')\n",
    "#makeGOHyperHeatmapPlots(overallDF, 'bo-ucb')\n",
    "#makeGOHyperHeatmapPlots(overallDF, 'bo-ei')\n",
    "#makeGOHyperHeatmapPlots(overallDF, 'bo-poi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we assume that the DF passed in here has the same method, progname and probsize\n",
    "def preprepareDF(df, method,  progname, probsize):\n",
    "\txaxis, yaxis = hypers[method]\n",
    "\t\n",
    "\taxisCols = list(xaxis+yaxis)\n",
    "\taxisCols.remove('seed')\n",
    "\tcolsToKeep = ['seed', 'globalSample']+axisCols\n",
    "\tcolsToDrop = list(df.columns)\n",
    "\t\n",
    "\t[colsToDrop.remove(col) for col in colsToKeep]\n",
    "\tcolsToDrop.remove('xtime')\n",
    "\t\n",
    "\t# get rid of unused columns\n",
    "\tdf = df.drop(colsToDrop, axis=1)\n",
    "\t\n",
    "\t# presort the DF\n",
    "\tdf = df.sort_values(by=colsToKeep, ignore_index=True)\n",
    "\n",
    "\t#print('keeping cols')\n",
    "\t#print(colsToKeep)\n",
    "\n",
    "\t#print('df now')\n",
    "\t#print(df.columns, '\\n', df.head())\n",
    "\t\n",
    "\t# pick one combination to make column tuples with\n",
    "\tsubset = df[(df['seed'] == seeds[0]) & (df['globalSample'] == 0)]\n",
    "\t\n",
    "\t# need to make tuples of the columns, and stringify them\n",
    "\tif len(xaxis) > 1:\n",
    "\t\txtuples = list(product(*[list(sorted(subset[col].unique())) for col in xaxis]))\n",
    "\t\n",
    "\t\t# tupleify the columns and drop them\n",
    "\t\tnewXColName = f'({\",\".join(xaxis)})'\n",
    "\t\tdf[newXColName] = list(zip(*[df[col] for col in xaxis]))\n",
    "\t\tdf = df.drop(list(xaxis), axis=1)\n",
    "\telse:\n",
    "\t\txtuples = list(sorted(subset[xaxis[0]].unique()))\n",
    "\t\tnewXColName = xaxis[0]\n",
    "\t\n",
    "\tif len(yaxis) > 1:\n",
    "\t\tytuples = list(product(*[list(sorted(subset[col].unique())) for col in yaxis]))\n",
    "\t\n",
    "\t\t# tupleify the columns and drop them\n",
    "\t\tnewYColName = f'({\",\".join(yaxis)})'\n",
    "\t\tdf[newYColName] = list(zip(*[df[col] for col in yaxis]))\n",
    "\t\tdf = df.drop(list(yaxis), axis=1)\n",
    "\telse:\n",
    "\t\tytuples = list(sorted(subset[yaxis[0]].unique()))\n",
    "\t\tnewYColName = yaxis[0]\n",
    "\t\n",
    "\t# find the min xtime found up to each globalSample\n",
    "\tdf['cummin'] = df.groupby([newXColName, newYColName])['xtime'].transform('cummin')\n",
    "\t\n",
    "\t# rescale the xtime to be baseline-normalized\n",
    "\tdf = df.set_index([newXColName, newYColName])\n",
    "\n",
    "\tbaselineXtime = globalBaselines[(globalBaselines.progname == progname) & (globalBaselines.probsize == probsize)]['xtime'].iat[0]\n",
    "\n",
    "\t#print('got baseline xtime', baselineXtime)\n",
    "\n",
    "\tdf['baselineXtime'] = baselineXtime/df['cummin']\n",
    "\n",
    "\t#print('new new df')\n",
    "\t#print(df.columns, '\\n', df.head())\n",
    "\n",
    "\treturn df, newXColName, newYColName\n",
    "\n",
    "def drawEarlySamplesHeatmap(*args, **kwargs):\n",
    "\tdf = kwargs.pop('data').copy(deep=True)\n",
    "\tmethod = df['method'].iat[0]\n",
    "\tprobsize = df['probsize'].iat[0]\n",
    "\tprogname = df['progname'].iat[0]\n",
    "\n",
    "\t#print(f'working on {method}, {probsize}, {progname}')\n",
    "\n",
    "\tdf, newXColName, newYColName = preprepareDF(df, method, progname, probsize)\n",
    "\n",
    "\tearliestSamples = pd.DataFrame(index=df.index.copy())\n",
    "\tearliestSamples = earliestSamples[~earliestSamples.index.duplicated(keep='first')]\n",
    "\tearliestSamples['firstSample'] = 301\n",
    "\ttest = df.loc[df.baselineXtime > 1.0].groupby(level=[0,1])['globalSample'].min()\n",
    "\tearliestSamples['firstSample'].update(test)\n",
    "\tearliestSamples = earliestSamples.reset_index()\n",
    "\n",
    "\tdf = earliestSamples\n",
    "\tdf = df.pivot(index=newYColName, columns=newXColName, values='firstSample')\n",
    "\n",
    "\t#print('after pivot')\n",
    "\t#print(df.columns, '\\n', df.head())\n",
    "\n",
    "\tif method == 'pso':\n",
    "\t\tsns.heatmap(df, annot_kws={'fontsize':6}, **kwargs)\n",
    "\telse:\n",
    "\t\tsns.heatmap(df, **kwargs)\n",
    "\n",
    "\treturn\n",
    "\n",
    "def drawSpeedupHeatmap(*args, **kwargs):\n",
    "\tdf = kwargs.pop('data').copy(deep=True)\n",
    "\tstep = kwargs.pop('step')\n",
    "\n",
    "\tmethod = df['method'].iat[0]\n",
    "\tprobsize = df['probsize'].iat[0]\n",
    "\tprogname = df['progname'].iat[0]\n",
    "\n",
    "\tdf, newXColName, newYColName = preprepareDF(df, method, progname, probsize)\n",
    "\n",
    "\tsamps = pd.DataFrame(index=df.index.copy())\n",
    "\tsamps = samps[~samps.index.duplicated(keep='first')]\n",
    "\n",
    "\tcolName = f'step{step}'\n",
    "\tsamps[colName] = 0\n",
    "\ttest = df.loc[df.globalSample == step].groupby(level=[0,1])['baselineXtime'].max()\n",
    "\tsamps[colName].update(test)\n",
    "\tsamps = samps.reset_index()\n",
    "\n",
    "\toptimalXtime = globalOptimals[(globalOptimals.progname == progname) & (globalOptimals.probsize == probsize)]['xtime'].iat[0]\n",
    "\tbaselineXtime = globalBaselines[(globalBaselines.progname == progname) & (globalBaselines.probsize == probsize)]['xtime'].iat[0]\n",
    "\n",
    "\tvmax = baselineXtime/optimalXtime\n",
    "\n",
    "\t#print(f'working on {probsize} {progname} vmax {vmax}', end=\"\\t\")\n",
    "\n",
    "\tsamps = samps.pivot(index=newYColName, columns=newXColName, values=colName)\n",
    "\n",
    "\tif method == 'pso':\n",
    "\t\tsns.heatmap(samps, norm=mcolors.TwoSlopeNorm(vcenter=1.0, vmin=0.0, vmax=vmax), cmap='seismic', annot_kws={'fontsize':6}, **kwargs)\n",
    "\telse:\n",
    "\t\tsns.heatmap(samps, norm=mcolors.TwoSlopeNorm(vcenter=1.0, vmin=0.0, vmax=vmax), cmap='seismic', **kwargs)\n",
    "\n",
    "\t#print('done!')\n",
    "\t\n",
    "\treturn\n",
    "\n",
    "def makeGOHyperHeatmapPlots(df, progname):\n",
    "\n",
    "\t# get the subset we're interested in\n",
    "\tdf = df[df['progname'] == progname].reset_index(drop=True)\n",
    "\n",
    "\t#print('raw data')\n",
    "\t#print(df.columns, df.head())\n",
    "\t\n",
    "\t# for each method we'll need to figure out the axes\n",
    "\t#for method in list(hypers.keys()):\n",
    "\t\n",
    "\t# let's first make the tuples of columns\n",
    "\tg = sns.FacetGrid(df, row='method', col='probsize', col_order=probsizes, height=15, aspect=1.5, sharex=False, sharey=False)\n",
    "\tg.map_dataframe(drawEarlySamplesHeatmap, annot=True, vmin=0, vmax=300, cbar=True, xticklabels=True, yticklabels=True, fmt='.3g')\n",
    "\t\n",
    "\tfor ax in g.axes.flatten():\n",
    "\t\tax.tick_params(axis='x', labelbottom=True, labelrotation=90)\n",
    "\t\n",
    "\t#plt.tight_layout()\n",
    "\tg.set_titles(col_template=\"Problem Size: {col_name}\", row_template=\"GO Method: {row_name}\")\n",
    "\t\n",
    "\tg.fig.subplots_adjust(top=0.96)\n",
    "\tg.fig.suptitle(f'GO Hyperparam Exploration ({progname.upper()}) -- Samples till > baseline xtime')\n",
    "\t\n",
    "\tplt.show()\n",
    "\n",
    "\t# let's make the plots showing what happens at each step 0, 100, 200, 299\n",
    "\t# want the best speedup witnessed up until each point\n",
    "\n",
    "\t# let's first make the tuples of columns\n",
    "\tfor i in range(1,4):\n",
    "\t\tstep = 100*i\n",
    "\t\tstep = 299 if step == 300 else step\n",
    "\n",
    "\t\tg = sns.FacetGrid(df, row='method', col='probsize', col_order=probsizes, height=15, aspect=1.5, sharex=False, sharey=False)\n",
    "\t\tg.map_dataframe(drawSpeedupHeatmap, step=step, annot=True, vmin=0.0, fmt='.3f', cbar=True, xticklabels=True, yticklabels=True)\n",
    "\n",
    "\t\tfor ax in g.axes.flatten():\n",
    "\t\t\tax.tick_params(axis='x', labelbottom=True, labelrotation=90)\n",
    "\n",
    "\t\tg.set_titles(col_template=\"Problem Size: {col_name}\", row_template=\"GO Method: {row_name}\")\n",
    "\t\t#plt.tight_layout()\n",
    "\n",
    "\t\tg.fig.subplots_adjust(top=0.96)\n",
    "\t\tg.fig.suptitle(f'GO Hyperparam Exploration ({progname.upper()}) -- Best Speedup after {step} samples')\n",
    "\n",
    "\t\tplt.show()\n",
    "\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make the plots showing what happens at each step 0, 50, 100, 150, 200, 250, 300\n",
    "# want the best speedup witnessed up until each point\n",
    "\n",
    "makeGOHyperHeatmapPlots(overallDF, 'bt_nas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "makeGOHyperHeatmapPlots(overallDF, 'ft_nas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "makeGOHyperHeatmapPlots(overallDF, 'hpcg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "makeGOHyperHeatmapPlots(overallDF, 'lulesh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to make one plot for each GO method\n",
    "# want to show programs in rows, probsize in columns\n",
    "# the heatmap for each will show the hyperparameters on the two axes\n",
    "\n",
    "# we're going to generate 4 heatmaps showing the following values:\n",
    "# 1) the earliest step a better-than-baseline configuration found\n",
    "# 2) the speedup of the best configuration found after 100 steps\n",
    "# 3) the speedup of the best configuration found after 200 steps\n",
    "# 4) the speedup of the best configuration found after 300 steps\n",
    "\n",
    "# get the subset we're interested in\n",
    "#method = 'cma'\n",
    "#df = overallDF[overallDF['method'] == method].reset_index(drop=True)\n",
    "#\n",
    "#print(df.columns)\n",
    "#\n",
    "#xaxis, yaxis = hypers[method]\n",
    "#print(xaxis, yaxis)\n",
    "#\n",
    "#axisCols = list(xaxis+yaxis)\n",
    "#axisCols.remove('seed')\n",
    "#colsToKeep = ['progname', 'probsize', 'seed', 'globalSample']+axisCols\n",
    "#colsToDrop = list(df.columns)\n",
    "#\n",
    "#[colsToDrop.remove(col) for col in colsToKeep]\n",
    "#colsToDrop.remove('xtime')\n",
    "#\n",
    "#print('dropping')\n",
    "#print(colsToDrop)\n",
    "#\n",
    "## get rid of unused columns\n",
    "#df = df.drop(colsToDrop, axis=1)\n",
    "#\n",
    "## presort the DF\n",
    "#df = df.sort_values(by=colsToKeep, ignore_index=True)\n",
    "#\n",
    "## pick one combination to make column tuples with\n",
    "#subset = df[(df['progname'] == prognames[0]) & (df['probsize'] == probsizes[0]) & (df['seed'] == seeds[0]) & (df['globalSample'] == 0)]\n",
    "#print('subset cols', subset.columns)\n",
    "#print('subset shape',subset.shape)\n",
    "#\n",
    "## need to make tuples of the columns, and stringify them\n",
    "#if len(xaxis) > 1:\n",
    "#\txtuples = list(product(*[list(sorted(subset[col].unique())) for col in xaxis]))\n",
    "#\t#tuples = list(set([ str(a) for a in list(zip( *[list(subset[col]) for col in xaxis] ))]))\n",
    "#\tprint(len(xtuples), xtuples)\n",
    "#\n",
    "#\t# tupleify the columns and drop them\n",
    "#\tnewXColName = f'({\",\".join(xaxis)})'\n",
    "#\tdf[newXColName] = list(zip(*[df[col] for col in xaxis]))\n",
    "#\tdf = df.drop(list(xaxis), axis=1)\n",
    "#else:\n",
    "#\txtuples = list(sorted(subset[xaxis[0]].unique()))\n",
    "#\tnewXColName = xaxis[0]\n",
    "#\tprint(xtuples)\n",
    "#\n",
    "#if len(yaxis) > 1:\n",
    "#\tytuples = list(product(*[list(sorted(subset[col].unique())) for col in yaxis]))\n",
    "#\t#tuples = list(set([ str(a) for a in list(zip( *[list(subset[col]) for col in xaxis] ))]))\n",
    "#\tprint(len(ytuples), ytuples)\n",
    "#\n",
    "#\t# tupleify the columns and drop them\n",
    "#\tnewYColName = f'({\",\".join(yaxis)})'\n",
    "#\tdf[newYColName] = list(zip(*[df[col] for col in yaxis]))\n",
    "#\tdf = df.drop(list(yaxis), axis=1)\n",
    "#else:\n",
    "#\tytuples = list(sorted(subset[yaxis[0]].unique()))\n",
    "#\tnewYColName = yaxis[0]\n",
    "#\tprint(ytuples)\n",
    "#\n",
    "#print('new columns')\n",
    "#print(df.columns)\n",
    "#\n",
    "##print('uniques')\n",
    "### for each column, print the number of unique values\n",
    "##for col in list(df.columns):\n",
    "##\tif col != 'xtime':\n",
    "##\t\tprint(col, len(list(df[col].unique())))\n",
    "#\n",
    "## find the min xtime found up to each globalSample\n",
    "#df['cummin'] = df.groupby([newXColName, newYColName, 'probsize', 'progname'])['xtime'].transform('cummin')\n",
    "#\n",
    "## let's make a new dataframe column for each plot type we want to make\n",
    "## 1) earliest better-than-baseline config found for each GO hyperparam combination\n",
    "#\n",
    "## rescale the xtime to be baseline-normalized\n",
    "#print('pre-reset index')\n",
    "#print(df.shape, df.head())\n",
    "#df = df.set_index(['progname', 'probsize', newXColName, newYColName])\n",
    "#baselines = globalBaselines.set_index(['progname', 'probsize'])\n",
    "#print('post- reset index')\n",
    "#print(df.shape, df.head())\n",
    "#\n",
    "#df['baselineXtime'] = 1/df['cummin'].div(baselines.reindex(df.index)['xtime'], axis=0)\n",
    "#\n",
    "#print('rescaled')\n",
    "#print(df.shape, df.head())\n",
    "#earliestSamples = pd.DataFrame(index=df.index.copy())\n",
    "##earliestSamples = df.copy(deep=True)\n",
    "#print('super init earliest sampels')\n",
    "#print(earliestSamples.shape, earliestSamples)\n",
    "#earliestSamples = earliestSamples[~earliestSamples.index.duplicated(keep='first')]\n",
    "##earliestSamples = earliestSamples.groupby(earliestSamples.index).first()\n",
    "#\n",
    "## set it to the latest possible value\n",
    "#earliestSamples['firstSample'] = 301\n",
    "#\n",
    "#print('init early samples')\n",
    "#print(earliestSamples.shape, earliestSamples)\n",
    "#\n",
    "##test = df.loc[df.baselineXtime >= 0.1, 'globalSample'].min()\n",
    "#test = df.loc[df.baselineXtime >= 1.0].groupby(level=[0,1,2,3])['globalSample'].min()\n",
    "#print('found earliest')\n",
    "#print(test.shape, test)\n",
    "#\n",
    "## now find the earliest globalSample that is >= 1.0\n",
    "## update only a couple elements\n",
    "#earliestSamples['firstSample'].update(test)\n",
    "#print('earliest samples')\n",
    "#print(earliestSamples.shape, earliestSamples)\n",
    "#\n",
    "## now reset the index\n",
    "#earliestSamples = earliestSamples.reset_index()\n",
    "#\n",
    "#print('reset index')\n",
    "#print(earliestSamples.shape, earliestSamples)\n",
    "\n",
    "\t\n",
    "#df['xtime'] = df.groupby([newXColName, newYColName, 'probsize', 'progname']).apply(getEarliestSampleBeatingBenchmark)['xtime']\n",
    "\n",
    "\n",
    "#df['reachedBaseline'] = df[xtime >= 1.0]\n",
    "\n",
    "#return\n",
    "#\n",
    "\n",
    "#def drawHeatmap(*args, **kwargs):\n",
    "#\tdata = kwargs.pop('data').copy(deep=True)\n",
    "#\tif 'probsize' in list(data.columns):\n",
    "#\t\tdata = data.drop(['probsize'], axis=1)\n",
    "#\tif 'progname' in list(data.columns):\n",
    "#\t\tdata = data.drop(['progname'], axis=1)\n",
    "#\tdata = data.pivot(index=newXColName, columns=newYColName, values='firstSample')\n",
    "#\t# silly how we have to resort this manually...\n",
    "#\t#data.index = pd.CategoricalIndex(data.index, categories=uniqA)\n",
    "#\t#data.sort_index(level=0, inplace=True)\n",
    "#\n",
    "#\t#data.columns = pd.CategoricalIndex(data.columns, categories=uniqB)\n",
    "#\t#data.sort_index(axis='columns', level='OMP_NUM_THREADS_PLACES', inplace=True)\n",
    "#\n",
    "#\t#print(data)\n",
    "#\t# plot the good values\n",
    "#\tax = sns.heatmap(data, **kwargs)\n",
    "#\treturn\n",
    "##\n",
    "##\n",
    "## let's first make the tuples of columns\n",
    "##\n",
    "#g = sns.FacetGrid(earliestSamples, row='progname', col='probsize', col_order=probsizes, palette='flare', height=10, aspect=1.5)\n",
    "##g.map_dataframe(drawHeatmap, annot=True, vmin=0.0, vmax=1.0, cbar=True)\n",
    "#g.map_dataframe(drawHeatmap, annot=False, vmin=0, vmax=300, cbar=True, xticklabels=True, yticklabels=True)\n",
    "##\n",
    "##\n",
    "#for ax in g.axes.flatten():\n",
    "#\tax.tick_params(axis='x', labelbottom=True, labelrotation=90)\n",
    "##\n",
    "#plt.tight_layout()\n",
    "##\n",
    "#g.fig.subplots_adjust(top=0.96)\n",
    "#g.fig.suptitle(f'GO Hyperparam Exploration ({method.upper()}) -- Samples till >= baseline xtime')\n",
    "##\n",
    "#plt.show()\n",
    "##\n",
    "## for PSO we will need to make tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make the plots showing what happens at each step 0, 50, 100, 150, 200, 250, 299\n",
    "# want the best speedup witnessed up until each point\n",
    "\n",
    "#samps = pd.DataFrame(index=df.index.copy())\n",
    "#samps = samps[~samps.index.duplicated(keep='first')]\n",
    "#\n",
    "#for i in range(1,7):\n",
    "#\tsteps = 50*i\n",
    "#\tsteps = 299 if steps == 300 else steps\n",
    "#\tcolName = f'step{steps}'\n",
    "#\tsamps[colName] = 0\n",
    "#\ttest = df.loc[df.globalSample == steps].groupby(level=[0,1,2,3])['baselineXtime'].max()\n",
    "#\tsamps[colName].update(test)\n",
    "#\n",
    "#samps = samps.reset_index()\n",
    "#\n",
    "#def drawHeatmap(*args, **kwargs):\n",
    "#\tdata = kwargs.pop('data').copy(deep=True)\n",
    "#\ttargetCol = kwargs.pop('step')\n",
    "#\tif 'probsize' in list(data.columns):\n",
    "#\t\tprobsize = data.iloc[0]['probsize']\n",
    "#\t\tdata = data.drop(['probsize'], axis=1)\n",
    "#\tif 'progname' in list(data.columns):\n",
    "#\t\tprogname = data.iloc[0]['progname']\n",
    "#\t\tdata = data.drop(['progname'], axis=1)\n",
    "#\t\t\n",
    "#\toptimalXtime = globalOptimals[(globalOptimals.progname == progname) & (globalOptimals.probsize == probsize)]['xtime'].iat[0]\n",
    "#\tbaselineXtime = globalBaselines[(globalBaselines.progname == progname) & (globalBaselines.probsize == probsize)]['xtime'].iat[0]\n",
    "#\n",
    "#\tvmax = baselineXtime/optimalXtime\n",
    "#\n",
    "#\t#print(f'working on {probsize} {progname} vmax {vmax}')\n",
    "#\n",
    "#\tdata = data.pivot(index=newXColName, columns=newYColName, values=targetCol)\n",
    "#\tax = sns.heatmap(data, vmax=vmax, **kwargs)\n",
    "#\treturn\n",
    "#\n",
    "#\n",
    "## let's first make the tuples of columns\n",
    "#for i in range(1,7):\n",
    "#\tstep = 50*i\n",
    "#\tstep = 299 if step == 300 else step\n",
    "#\n",
    "#\tg = sns.FacetGrid(samps, row='progname', col='probsize', col_order=probsizes, palette='flare', height=10, aspect=1.5)\n",
    "#\tcolName = f'step{step}'\n",
    "#\tg.map_dataframe(drawHeatmap, step=colName, annot=True, vmin=1.0, cbar=True, xticklabels=True, yticklabels=True)\n",
    "#\n",
    "#\tfor ax in g.axes.flatten():\n",
    "#\t\tax.tick_params(axis='x', labelbottom=True, labelrotation=90)\n",
    "#\n",
    "#\tplt.tight_layout()\n",
    "#\n",
    "#\tg.fig.subplots_adjust(top=0.96)\n",
    "#\tg.fig.suptitle(f'GO Hyperparam Exploration ({method.upper()}) -- Best Speedup at {step} Samples')\n",
    "#\n",
    "#\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#def makeOverallPlots(df, baselineXtimes=None):\n",
    "#\t# want each row to be a program, and each column to be a problem size\n",
    "#\t# each plot will show the average cumsum across each bo-util fnct, PSO, and CMA\n",
    "#\n",
    "#\totherCols = list(df.columns)\n",
    "#\totherCols.remove('xtime')\n",
    "#\totherCols.remove('globalSample')\n",
    "#\tprint(otherCols)\n",
    "#\n",
    "#\t# for each category, we need to do a cumulative max\n",
    "#\tdf['cummax'] = df.groupby(otherCols, dropna=False)['xtime'].transform('cummax')\n",
    "#\n",
    "#\tdef addBaselines(*args, **kwargs):\n",
    "#\t\tdata = kwargs['data']\n",
    "#\t\tprobsize = data.iloc[0]['probsize']\n",
    "#\t\tprogname = data.iloc[0]['progname']\n",
    "#\t\tmethod = data.iloc[0]['method']\n",
    "#\n",
    "#\t\tax = plt.gca()\n",
    "#\t\thandles, labels = ax.get_legend_handles_labels()\n",
    "#\n",
    "#\t\toptimal = globalOptimals[(globalOptimals['progname'] == progname) & (globalOptimals['probsize'] == probsize)]['xtime'].iat[0]\n",
    "#\t\tbaseline = globalBaselines[(globalBaselines['progname'] == progname) & (globalBaselines['probsize'] == probsize)]['xtime'].iat[0]\n",
    "#\n",
    "#\t\t#avrgCummax = data.groupby(['globalSample'], dropna=False)['cummax'].mean().reset_index()\n",
    "#\t\t#if baselineXtimes is None:\n",
    "#\t\t#\t#step = data[data['cummax'] >= 0.5]['globalSample'].min()\n",
    "#\t\t#\tstep = avrgCummax[avrgCummax['cummax'] >= 0.5]['globalSample'].min()\n",
    "#\t\t#\tif step is None:\n",
    "#\t\t#\t\tprint(f'{progname} {probsize} {method} did not reach baseline')\n",
    "#\t\t#\telse:\n",
    "#\t\t#\t\tprint(f'{progname} {probsize} {method} passed baseline on step {step}')\n",
    "#\t\t#else:\n",
    "#\t\t#\tval = baselineXtimes.loc[(baselineXtimes['progname'] == progname) & (baselineXtimes['probsize'] == probsize), 'xtime'].iat[0]\n",
    "#\t\t#\tposs = baselineXtimes.loc[(baselineXtimes['progname'] == progname) & (baselineXtimes['probsize'] == probsize), 'xtime']\n",
    "#\t\t#\tprint('poss')\n",
    "#\t\t#\tprint(poss)\n",
    "#\t\t#\tstep = avrgCummax[avrgCummax['cummax'] >= val]['globalSample'].min()\n",
    "#\t\t#\tif step is None:\n",
    "#\t\t#\t\tprint(f'{progname} {probsize} {method} did not reach baseline')\n",
    "#\t\t#\telse:\n",
    "#\t\t#\t\tprint(f'{progname} {probsize} {method} passed baseline on step {step}')\n",
    "#\t\t#\tprint('at step 280, avrgCummax is', avrgCummax[avrgCummax['globalSample'] == 280.0])\n",
    "#\n",
    "#\t\tif not ('optimal' in labels):\n",
    "#\t\t\tax.set_title(f'{prognameMap[progname]} -- {probsizeMap[probsize]}')\n",
    "#\t\t\tax.set_xlabel('Sample Index')\n",
    "#\n",
    "#\t\t\tif baselineXtimes is None:\n",
    "#\t\t\t\t#ax.set_ylabel('Norm. to Optimal/Baseline Execution Times from Database \\n(higher is better)')\n",
    "#\t\t\t\tax.axhline(baseline/optimal, c='blue', linestyle='--', zorder=0, label='optimal')\n",
    "#\t\t\t\tax.axhline(1.0, c='red', linestyle='--', zorder=0, label='baseline')\n",
    "#\t\t\t\t# what step do we pass the baseline xtime?\n",
    "#\t\t\telse:\n",
    "#\t\t\t\t#ax.set_ylabel('Norm. to Optimal/Worst Execution Times from Database \\n(higher is better)')\n",
    "#\t\t\t\tval = baselineXtimes.loc[(baselineXtimes['progname'] == progname) & (baselineXtimes['probsize'] == probsize), 'xtime'].iat[0]\n",
    "#\t\t\t\tax.axhline(1.0, c='blue', linestyle='--', zorder=0, label='optimal')\n",
    "#\t\t\t\tax.axhline(val, c='red', linestyle='--', zorder=0, label='baseline')\n",
    "#\n",
    "#\t\t#ax.legend(loc='lower right')\n",
    "#\t\treturn\n",
    "#\n",
    "#\tg = sns.FacetGrid(df, row='progname', col='probsize', col_order=probsizes, hue='method', \n",
    "#\t\t\t\t\t\t\t\t\t\tpalette='flare', legend_out=False, height=5, aspect=1.5, sharex=False, sharey=False)\n",
    "#\tg.map_dataframe(sns.lineplot, x='globalSample', y='cummax', errorbar=\"pi\")\n",
    "#\tg.map_dataframe(addBaselines)\n",
    "#\t#g.set(ylim=(-0.05, 1.05))\n",
    "#\n",
    "#\taxes = g.axes\n",
    "#\tfor r in range(axes.shape[0]):\n",
    "#\t\tif baselineXtimes is None:\n",
    "#\t\t\taxes[r,0].set_ylabel('Norm. to Baseline from Database (Speedup) \\n( >1.0 is better )')\n",
    "#\t\telse:\n",
    "#\t\t\taxes[r,0].set_ylabel('Norm. to Optimal/Worst Execution Times from Database \\n(higher is better)')\n",
    "#\t\tfor c in range(axes.shape[1]):\n",
    "#\t\t\taxes[r,c].legend(loc='lower right')\n",
    "#\n",
    "#\tplt.tight_layout()\n",
    "#\n",
    "#\tg.fig.subplots_adjust(top=0.95)\n",
    "#\tg.fig.suptitle('Average highest normalized execution time found at each optimization step')\n",
    "#\n",
    "#\tplt.show()\n",
    "#\treturn\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#useBaseline=True\n",
    "#\n",
    "#overallDF = pd.DataFrame()\n",
    "#tojoin = []\n",
    "#for progname in prognames:\n",
    "#\tif progname == 'cg_nas' or progname == 'cfd_rodinia':\n",
    "#\t\tcontinue\n",
    "#\tfor method in goMethods:\n",
    "#\t\t# read the pre-processed dataframe\n",
    "#\t\tif useBaseline:\n",
    "#\t\t\tfilename = ROOT_DIR+'/databases/'+f'{MACHINE}-{progname}-{method}-GO_Data-baselineNorm.csv'\n",
    "#\t\telse:\n",
    "#\t\t\tfilename = ROOT_DIR+'/databases/'+f'{MACHINE}-{progname}-{method}-GO_Data.csv'\n",
    "#\t\tfullDF = pd.read_csv(filename)\n",
    "#\t\ttojoin += [fullDF]\n",
    "#\n",
    "#overallDF = pd.concat(tojoin, ignore_index=True, sort=True)\n",
    "#overallDF = overallDF.drop(['optimXtime', 'kappa_decay', 'kappa_decay_delay'], axis=1)\n",
    "#\n",
    "#print(overallDF.columns)\n",
    "## for the method column, make all the bo entries a union of the method and utilFnct\n",
    "##overallDF[overallDF['method'] == 'bo'].apply(lambda x: x['method']+'-'+x['utilFnct'], axis=1)\n",
    "##print('it works')\n",
    "#overallDF.loc[overallDF['method'] == 'bo', 'method'] = overallDF[overallDF['method'] == 'bo'].apply(lambda x: x['method']+'-'+x['utilFnct'], axis=1)\n",
    "##print(overallDF['method'].unique())\n",
    "#\n",
    "#overallDF = overallDF.drop(['utilFnct'], axis=1)\n",
    "#\n",
    "#for col in overallDF:\n",
    "#\tif col == 'xtime' or col == 'globalSample' or col == 'optimXtime':\n",
    "#\t\tcontinue\n",
    "#\tprint(col, overallDF[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baselinesDF = xtimeDB[(xtimeDB['OMP_NUM_THREADS'] == numthreads) \n",
    "#\t\t\t\t\t\t\t\t\t\t& (xtimeDB['OMP_PROC_BIND'] == 'close')\n",
    "#\t\t\t\t\t\t\t\t\t\t& (xtimeDB['OMP_PLACES'] == 'threads')\n",
    "#\t\t\t\t\t\t\t\t\t\t& (xtimeDB['OMP_SCHEDULE'] == 'static')]\n",
    "#\n",
    "#def normToMinMax(row):\n",
    "#\tprogname = row['progname']\n",
    "#\tprobsize = row['probsize']\n",
    "#\n",
    "#\tminVal, maxVal = getMinMaxXtimeForProg(progname, probsize)\n",
    "#\n",
    "#\trow['xtime'] = 1 - (row['xtime']-minVal)/(maxVal-minVal)\n",
    "#\treturn row\n",
    "#\n",
    "#\n",
    "## normalize the baselinesDF to the min/max\n",
    "#baselinesDF.loc[:,'xtime'] = baselinesDF.apply(normToMinMax, axis=1)['xtime']\n",
    "#\n",
    "#if useBaseline:\n",
    "#\tmakeOverallPlots(overallDF)\n",
    "#else:\n",
    "#\tmakeOverallPlots(overallDF,baselinesDF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10.8-gregvirtenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
